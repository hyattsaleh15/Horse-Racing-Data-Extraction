{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import csv\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2018-07-01\"\n",
    "end = \"2018-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of string to datetime objects\n",
    "start_date = datetime.datetime.strptime(start, \"%Y-%m-%d\")\n",
    "end_date = datetime.datetime.strptime(end, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output file is created with headers\n",
    "headers = [\"name\",\"date\",\"time\",\"going\",\"surface\",\"url\"]\n",
    "\n",
    "with open('links.csv','w',newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2018-07-01 00:00:00\n",
      "Performing requests, please wait...\n",
      "\tCurragh\n",
      "\tWindsor\n",
      "\tCartmel\n",
      "\tUttoxeter\n",
      "\tHamburg\n",
      "\tHassloch\n",
      "\tSaint-Cloud\n",
      "\tGulfstream\n",
      "\tLos Alamitos Racecourse\n",
      "\tMonmouth Park\n",
      "\tMountaineer\n",
      "\tOak Tree At Pleasanton\n",
      "\tWoodbine\n"
     ]
    }
   ],
   "source": [
    "# The while operator makes sure to only extract information between the start and end dates\n",
    "while start_date <= end_date:\n",
    "    time.sleep(5)\n",
    "    print (\"\\n\", start_date)\n",
    "    \n",
    "    # Construct the URL for each day and perform the request\n",
    "    url = \"https://www.sportinglife.com/racing/racecards/\" + str(start_date.year) + \"-\" + str(start_date.month) + \"-\" + str(start_date.day)\n",
    "    print(\"Performing requests, please wait...\")\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    date_or = str(start_date.year) + \"-\" + str(start_date.month) + \"-\" + str(start_date.day)\n",
    "\n",
    "    # Using xpaths, navigate through the content of the site to get to the section that lists all locations and races\n",
    "    places = tree.xpath('//div[@class=\"collapsibleSection\"]')\n",
    "\n",
    "    # This for loop goes through each location in which was a race during that day\n",
    "    for k in places:\n",
    "        urls = []\n",
    "        \n",
    "        # There are some races that were cancelled or rescheduled. This if makes sure to grab the URL of those races that were run\n",
    "        if k.xpath('div[@class=\"dividerRow\"]/h2/a[not(contains(text(),\"OFF\"))]') and k.xpath('div[2]/ul/li[1]/div[@class=\"hr-meeting-race-result-fulllink\"]'):\n",
    "            \n",
    "            # Using xpaths, get the name, going and surface of each location\n",
    "            name = k.xpath('div[@class=\"dividerRow\"]/h2/a/text()')[0]\n",
    "            going = k.xpath('div[2]/div/div[@class=\"hr-meeting-meta-going\"]/span[@class=\"hr-meeting-meta-value\"]/text()')[0]\n",
    "            surface = k.xpath('div[2]/div/div[@class=\"hr-meeting-meta-surface\"]/span/text()')[0]\n",
    "            print(\"\\t\" + name)\n",
    "\n",
    "            # Perform a for loop through the races in each location\n",
    "            races = k.xpath('div[2]/ul/li')\n",
    "            for l in races:\n",
    "                \n",
    "                # Use xpaths to get the url and the time of each race\n",
    "                link = \"https://www.sportinglife.com\" + l.xpath('a/@href')[0]\n",
    "                time_or = l.xpath('a/span/text()')[0]\n",
    "                \n",
    "                # For each race, append the extracted information to a list\n",
    "                urls.append([name,date_or,time_or,going,surface,link])\n",
    "        \n",
    "        # Write the list of URLs and race information to the csv file. This operation is done for every location of every day\n",
    "        with open('links.csv','a',newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(urls)\n",
    "    \n",
    "    # Increment the start_date variable by one day\n",
    "    start_date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
